{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdS6uKytHG+bQ5tRss29rU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrescoDev/simple-neural-network-demo/blob/main/simple_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Neural Network (demo)\n",
        "\n",
        "A simplified NN put together to illustrate core ML concepts using core Python and NumPy"
      ],
      "metadata": {
        "id": "TsA8ku2XsTEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's import NumPy in order to allow us to easily work with arrays and matrices."
      ],
      "metadata": {
        "id": "tYgpC9bIsjH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "D5fepk2Ls-8K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a simple neural network with:\n",
        "\n",
        "- 2 input neurons\n",
        "- 1 hidden layer with 3 neurons\n",
        "- 1 output node with 1 neuron\n",
        "\n",
        "This kind of looks like:\n",
        "\n",
        "![graphviz.svg](data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiIHN0YW5kYWxvbmU9Im5vIj8+PCFET0NUWVBFIHN2ZyBQVUJMSUMgIi0vL1czQy8vRFREIFNWRyAxLjEvL0VOIiAiaHR0cDovL3d3dy53My5vcmcvR3JhcGhpY3MvU1ZHLzEuMS9EVEQvc3ZnMTEuZHRkIj48IS0tIEdlbmVyYXRlZCBieSBncmFwaHZpeiB2ZXJzaW9uIDIuNDAuMSAoMjAxNjEyMjUuMDMwNCkKIC0tPjwhLS0gVGl0bGU6IEcgUGFnZXM6IDEgLS0+PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIzcHQiIGhlaWdodD0iMzI2cHQiIHZpZXdCb3g9IjAuMDAgMC4wMCAzMjMuNDEgMzI2LjQ2Ij4KPGcgaWQ9ImdyYXBoMCIgY2xhc3M9ImdyYXBoIiB0cmFuc2Zvcm09InNjYWxlKDEgMSkgcm90YXRlKDApIHRyYW5zbGF0ZSg0IDMyMi40NjAxKSI+Cjx0aXRsZT5HPC90aXRsZT4KPHBvbHlnb24gZmlsbD0iI2ZmZmZmZiIgc3Ryb2tlPSJ0cmFuc3BhcmVudCIgcG9pbnRzPSItNCw0IC00LC0zMjIuNDYwMSAzMTkuNDEyNywtMzIyLjQ2MDEgMzE5LjQxMjcsNCAtNCw0Ii8+CjwhLS0gMCAtLT4KPGcgaWQ9Im5vZGUxIiBjbGFzcz0ibm9kZSI+Cjx0aXRsZT4wPC90aXRsZT4KPGVsbGlwc2UgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjMDAwMDAwIiBjeD0iMTA5LjcwNjQiIGN5PSItMjc5Ljc5NyIgcng9IjM4LjgyNjkiIHJ5PSIzOC44MjY5Ii8+Cjx0ZXh0IHRleHQtYW5jaG9yPSJtaWRkbGUiIHg9IjEwOS43MDY0IiB5PSItMjc1LjU5NyIgZm9udC1mYW1pbHk9IlRpbWVzLHNlcmlmIiBmb250LXNpemU9IjE0LjAwIiBmaWxsPSIjMDAwMDAwIj5JbnB1dCAxPC90ZXh0Pgo8L2c+CjwhLS0gMiAtLT4KPGcgaWQ9Im5vZGUzIiBjbGFzcz0ibm9kZSI+Cjx0aXRsZT4yPC90aXRsZT4KPGVsbGlwc2UgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjMDAwMDAwIiBjeD0iNDYuNzA2NCIgY3k9Ii0xNTguNDI3NiIgcng9IjQ2LjkxMzYiIHJ5PSI0Ni45MTM2Ii8+Cjx0ZXh0IHRleHQtYW5jaG9yPSJtaWRkbGUiIHg9IjQ2LjcwNjQiIHk9Ii0xNTQuMjI3NiIgZm9udC1mYW1pbHk9IlRpbWVzLHNlcmlmIiBmb250LXNpemU9IjE0LjAwIiBmaWxsPSIjMDAwMDAwIj5IaWRkZW4gMTwvdGV4dD4KPC9nPgo8IS0tIDAmIzQ1OyZndDsyIC0tPgo8ZyBpZD0iZWRnZTEiIGNsYXNzPSJlZGdlIj4KPHRpdGxlPjAtJmd0OzI8L3RpdGxlPgo8cGF0aCBmaWxsPSJub25lIiBzdHJva2U9IiMwMDAwMDAiIGQ9Ik05MS44MjE2LC0yNDUuMzQyMUM4Ni4wMjM5LC0yMzQuMTcyNyA3OS40NjQxLC0yMjEuNTM1NCA3My4xNzg1LC0yMDkuNDI2MiIvPgo8cG9seWdvbiBmaWxsPSIjMDAwMDAwIiBzdHJva2U9IiMwMDAwMDAiIHBvaW50cz0iNzYuMTU2MSwtMjA3LjU2NTMgNjguNDQyNSwtMjAwLjMwMjMgNjkuOTQzMiwtMjEwLjc5MDMgNzYuMTU2MSwtMjA3LjU2NTMiLz4KPC9nPgo8IS0tIDMgLS0+CjxnIGlkPSJub2RlNCIgY2xhc3M9Im5vZGUiPgo8dGl0bGU+MzwvdGl0bGU+CjxlbGxpcHNlIGZpbGw9Im5vbmUiIHN0cm9rZT0iIzAwMDAwMCIgY3g9IjE1Ny43MDY0IiBjeT0iLTE1OC40Mjc2IiByeD0iNDYuOTEzNiIgcnk9IjQ2LjkxMzYiLz4KPHRleHQgdGV4dC1hbmNob3I9Im1pZGRsZSIgeD0iMTU3LjcwNjQiIHk9Ii0xNTQuMjI3NiIgZm9udC1mYW1pbHk9IlRpbWVzLHNlcmlmIiBmb250LXNpemU9IjE0LjAwIiBmaWxsPSIjMDAwMDAwIj5IaWRkZW4gMjwvdGV4dD4KPC9nPgo8IS0tIDAmIzQ1OyZndDszIC0tPgo8ZyBpZD0iZWRnZTIiIGNsYXNzPSJlZGdlIj4KPHRpdGxlPjAtJmd0OzM8L3RpdGxlPgo8cGF0aCBmaWxsPSJub25lIiBzdHJva2U9IiMwMDAwMDAiIGQ9Ik0xMjMuOTc4LC0yNDMuNzEwOEMxMjcuOTk2MywtMjMzLjU1MDQgMTMyLjQ1MjUsLTIyMi4yODI2IDEzNi43NzM5LC0yMTEuMzU1OSIvPgo8cG9seWdvbiBmaWxsPSIjMDAwMDAwIiBzdHJva2U9IiMwMDAwMDAiIHBvaW50cz0iMTQwLjA5OTMsLTIxMi40NjQyIDE0MC41MjI0LC0yMDEuODc3OCAxMzMuNTg5OSwtMjA5Ljg4OTggMTQwLjA5OTMsLTIxMi40NjQyIi8+CjwvZz4KPCEtLSA0IC0tPgo8ZyBpZD0ibm9kZTUiIGNsYXNzPSJub2RlIj4KPHRpdGxlPjQ8L3RpdGxlPgo8ZWxsaXBzZSBmaWxsPSJub25lIiBzdHJva2U9IiMwMDAwMDAiIGN4PSIyNjguNzA2NCIgY3k9Ii0xNTguNDI3NiIgcng9IjQ2LjkxMzYiIHJ5PSI0Ni45MTM2Ii8+Cjx0ZXh0IHRleHQtYW5jaG9yPSJtaWRkbGUiIHg9IjI2OC43MDY0IiB5PSItMTU0LjIyNzYiIGZvbnQtZmFtaWx5PSJUaW1lcyxzZXJpZiIgZm9udC1zaXplPSIxNC4wMCIgZmlsbD0iIzAwMDAwMCI+SGlkZGVuIDM8L3RleHQ+CjwvZz4KPCEtLSAwJiM0NTsmZ3Q7NCAtLT4KPGcgaWQ9ImVkZ2UzIiBjbGFzcz0iZWRnZSI+Cjx0aXRsZT4wLSZndDs0PC90aXRsZT4KPHBhdGggZmlsbD0ibm9uZSIgc3Ryb2tlPSIjMDAwMDAwIiBkPSJNMTM5LjA5ODQsLTI1NC42Nzc5QzE0NC44Mzc3LC0yNTAuMDQzNCAxNTAuODgzNCwtMjQ1LjM0MjcgMTU2LjcwNjQsLTI0MS4xMzM5IDE4MC45OTAzLC0yMjMuNTgyMSAxODkuNTYwNSwtMjIyLjg3NTMgMjEzLjcwNjQsLTIwNS4xMzM5IDIxNy42MDI0LC0yMDIuMjcxMyAyMjEuNTczOCwtMTk5LjIxMDIgMjI1LjUwOTIsLTE5Ni4wNzU3Ii8+Cjxwb2x5Z29uIGZpbGw9IiMwMDAwMDAiIHN0cm9rZT0iIzAwMDAwMCIgcG9pbnRzPSIyMjcuOTQ5NywtMTk4LjYwMjMgMjMzLjUwMSwtMTg5LjU3ODMgMjIzLjUzMzgsLTE5My4xNzA5IDIyNy45NDk3LC0xOTguNjAyMyIvPgo8L2c+CjwhLS0gMSAtLT4KPGcgaWQ9Im5vZGUyIiBjbGFzcz0ibm9kZSI+Cjx0aXRsZT4xPC90aXRsZT4KPGVsbGlwc2UgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjMDAwMDAwIiBjeD0iMjA0LjcwNjQiIGN5PSItMjc5Ljc5NyIgcng9IjM4LjgyNjkiIHJ5PSIzOC44MjY5Ii8+Cjx0ZXh0IHRleHQtYW5jaG9yPSJtaWRkbGUiIHg9IjIwNC43MDY0IiB5PSItMjc1LjU5NyIgZm9udC1mYW1pbHk9IlRpbWVzLHNlcmlmIiBmb250LXNpemU9IjE0LjAwIiBmaWxsPSIjMDAwMDAwIj5JbnB1dCAyPC90ZXh0Pgo8L2c+CjwhLS0gMSYjNDU7Jmd0OzIgLS0+CjxnIGlkPSJlZGdlNCIgY2xhc3M9ImVkZ2UiPgo8dGl0bGU+MS0mZ3Q7MjwvdGl0bGU+CjxwYXRoIGZpbGw9Im5vbmUiIHN0cm9rZT0iIzAwMDAwMCIgZD0iTTE3NS4wMzM2LC0yNTQuNTExQzE2OS4zNzI3LC0yNDkuOTM5MiAxNjMuNDI3MSwtMjQ1LjMwMzcgMTU3LjcwNjQsLTI0MS4xMzM5IDEzMy43OTU5LC0yMjMuNzA1NiAxMjUuNDc5NywtMjIyLjc0ODggMTAxLjcwNjQsLTIwNS4xMzM5IDk3LjgyMTgsLTIwMi4yNTU3IDkzLjg1ODUsLTE5OS4xODM3IDg5LjkyODUsLTE5Ni4wNDE5Ii8+Cjxwb2x5Z29uIGZpbGw9IiMwMDAwMDAiIHN0cm9rZT0iIzAwMDAwMCIgcG9pbnRzPSI5MS45MDY5LC0xOTMuMTM5MSA4MS45NDQxLC0xODkuNTM0NCA4Ny40ODQ0LC0xOTguNTY1MiA5MS45MDY5LC0xOTMuMTM5MSIvPgo8L2c+CjwhLS0gMSYjNDU7Jmd0OzMgLS0+CjxnIGlkPSJlZGdlNSIgY2xhc3M9ImVkZ2UiPgo8dGl0bGU+MS0mZ3Q7MzwvdGl0bGU+CjxwYXRoIGZpbGw9Im5vbmUiIHN0cm9rZT0iIzAwMDAwMCIgZD0iTTE5MC43MzIxLC0yNDMuNzEwOEMxODYuODM5MywtMjMzLjY1ODUgMTgyLjUyNjksLTIyMi41MjIyIDE3OC4zMzc4LC0yMTEuNzA0NyIvPgo8cG9seWdvbiBmaWxsPSIjMDAwMDAwIiBzdHJva2U9IiMwMDAwMDAiIHBvaW50cz0iMTgxLjU3NywtMjEwLjM3NzEgMTc0LjcwMiwtMjAyLjMxNTggMTc1LjA0OTQsLTIxMi45MDQ5IDE4MS41NzcsLTIxMC4zNzcxIi8+CjwvZz4KPCEtLSAxJiM0NTsmZ3Q7NCAtLT4KPGcgaWQ9ImVkZ2U2IiBjbGFzcz0iZWRnZSI+Cjx0aXRsZT4xLSZndDs0PC90aXRsZT4KPHBhdGggZmlsbD0ibm9uZSIgc3Ryb2tlPSIjMDAwMDAwIiBkPSJNMjIyLjg3NSwtMjQ1LjM0MjFDMjI4LjgyMzcsLTIzNC4wNjEgMjM1LjU2MiwtMjIxLjI4MjUgMjQyLjAwNTUsLTIwOS4wNjMxIi8+Cjxwb2x5Z29uIGZpbGw9IiMwMDAwMDAiIHN0cm9rZT0iIzAwMDAwMCIgcG9pbnRzPSIyNDUuMjg4OSwtMjEwLjM0IDI0Ni44NTc0LC0xOTkuODYxOSAyMzkuMDk3LC0yMDcuMDc0OSAyNDUuMjg4OSwtMjEwLjM0Ii8+CjwvZz4KPCEtLSA1IC0tPgo8ZyBpZD0ibm9kZTYiIGNsYXNzPSJub2RlIj4KPHRpdGxlPjU8L3RpdGxlPgo8ZWxsaXBzZSBmaWxsPSJub25lIiBzdHJva2U9IiMwMDAwMDAiIGN4PSIxNTcuNzA2NCIgY3k9Ii0zNy44NjA2IiByeD0iMzcuNzIxNyIgcnk9IjM3LjcyMTciLz4KPHRleHQgdGV4dC1hbmNob3I9Im1pZGRsZSIgeD0iMTU3LjcwNjQiIHk9Ii0zMy42NjA2IiBmb250LWZhbWlseT0iVGltZXMsc2VyaWYiIGZvbnQtc2l6ZT0iMTQuMDAiIGZpbGw9IiMwMDAwMDAiPk91dHB1dDwvdGV4dD4KPC9nPgo8IS0tIDImIzQ1OyZndDs1IC0tPgo8ZyBpZD0iZWRnZTciIGNsYXNzPSJlZGdlIj4KPHRpdGxlPjItJmd0OzU8L3RpdGxlPgo8cGF0aCBmaWxsPSJub25lIiBzdHJva2U9IiMwMDAwMDAiIGQ9Ik03OC41MTQ1LC0xMjMuODc3OUM5My4wNDg4LC0xMDguMDkwOSAxMTAuMjUxOCwtODkuNDA1MiAxMjQuOTA0NSwtNzMuNDg5NiIvPgo8cG9seWdvbiBmaWxsPSIjMDAwMDAwIiBzdHJva2U9IiMwMDAwMDAiIHBvaW50cz0iMTI3LjY3MTcsLTc1LjY1MTQgMTMxLjg3LC02NS45MjM4IDEyMi41MjE5LC03MC45MTAxIDEyNy42NzE3LC03NS42NTE0Ii8+CjwvZz4KPCEtLSAzJiM0NTsmZ3Q7NSAtLT4KPGcgaWQ9ImVkZ2U4IiBjbGFzcz0iZWRnZSI+Cjx0aXRsZT4zLSZndDs1PC90aXRsZT4KPHBhdGggZmlsbD0ibm9uZSIgc3Ryb2tlPSIjMDAwMDAwIiBkPSJNMTU3LjcwNjQsLTExMS40NjhDMTU3LjcwNjQsLTEwMy4xMDA4IDE1Ny43MDY0LC05NC4zNzMxIDE1Ny43MDY0LC04NS45NzQyIi8+Cjxwb2x5Z29uIGZpbGw9IiMwMDAwMDAiIHN0cm9rZT0iIzAwMDAwMCIgcG9pbnRzPSIxNjEuMjA2NSwtODUuOTM4OSAxNTcuNzA2NCwtNzUuOTM4OSAxNTQuMjA2NSwtODUuOTM4OSAxNjEuMjA2NSwtODUuOTM4OSIvPgo8L2c+CjwhLS0gNCYjNDU7Jmd0OzUgLS0+CjxnIGlkPSJlZGdlOSIgY2xhc3M9ImVkZ2UiPgo8dGl0bGU+NC0mZ3Q7NTwvdGl0bGU+CjxwYXRoIGZpbGw9Im5vbmUiIHN0cm9rZT0iIzAwMDAwMCIgZD0iTTIzNi44OTgyLC0xMjMuODc3OUMyMjIuMzYzOSwtMTA4LjA5MDkgMjA1LjE2MDksLTg5LjQwNTIgMTkwLjUwODIsLTczLjQ4OTYiLz4KPHBvbHlnb24gZmlsbD0iIzAwMDAwMCIgc3Ryb2tlPSIjMDAwMDAwIiBwb2ludHM9IjE5Mi44OTA5LC03MC45MTAxIDE4My41NDI3LC02NS45MjM4IDE4Ny43NDEsLTc1LjY1MTQgMTkyLjg5MDksLTcwLjkxMDEiLz4KPC9nPgo8L2c+Cjwvc3ZnPg==)\n",
        "\n",
        "Each of the arrows represent a weight in the neural network "
      ],
      "metadata": {
        "id": "0w6JhvkatCqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define a class to represent our neural network. \n",
        "\n",
        "The main components will include:\n",
        "\n",
        "- the nodes and the weights between the nodes, and the learning rate\n",
        "- the train method is the main method that will be called to train the neural network\n",
        "- the predict method will be called to make predictions\n",
        "- the sigmoid method as the activation function that is applied to the signals"
      ],
      "metadata": {
        "id": "gHcCVid8TyYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by defining the weight matrices. There are two we'll need to build to connect the three layers:\n",
        "\n",
        "- Input -> Hidden\n",
        "- Hidden -> Output\n",
        "\n",
        "For the input to hidden weight matrix, this will look roughly like the following:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "   w_1 & w_2 & w_3 \\\\\n",
        "   w_4 & w_5 & w_6\n",
        "\\end{bmatrix}\n",
        "\n",
        "& the hidden to output weight matrix will look like:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "   w_1 \\\\\n",
        "   w_2 \\\\\n",
        "   w_3\n",
        "\\end{bmatrix}"
      ],
      "metadata": {
        "id": "KaiHi9hhZCFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to train the network, we'll go through the following process:\n",
        "\n",
        "- Forward pass: which specifies how the input data is transformed by the model to produce the output predictions.\n",
        "- Backward pass: which measures the difference between the predicted output and the true output and then update the model parameters based on the loss.\n",
        "\n",
        "\n",
        "\n",
        "Let's initialise some mock training data as follows:\n"
      ],
      "metadata": {
        "id": "H1e0ZNxkb0gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "train_outputs = np.array([[0, 1, 1, 0]]).T"
      ],
      "metadata": {
        "id": "rdQfRLJGdwFq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To break this down:\n",
        "\n",
        "- The first column is the input for the first node\n",
        "- The second column is the input for the second node\n",
        "- We have 4 training examples (or classes) denoted by the number of rows\n",
        "- The first row is the first training example\n",
        "- The second row is the second training example\n",
        "- The third row is the third training example\n",
        "- The fourth row is the fourth training example\n",
        "\n",
        "This looks like this in matrix form: \n",
        "\n",
        "\\begin{bmatrix}\n",
        "   0 & 0 \\\\\n",
        "   0 & 1 \\\\\n",
        "   1 & 0 \\\\\n",
        "   1 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "Where `(0,0)` would be our first training example, `(0,1)` our 2nd and so forth. \n",
        "\n",
        "A real world example of this could be a neural network that takes two inputs, height and weight, and outputs whether or not the person is overweight.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "   180 & 75 \\\\\n",
        "   190 & 85 \\\\\n",
        "   170 & 60 \\\\\n",
        "   180 & 90 \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "Where 180 cm and 75 kg would be the first training example.\n",
        "\n",
        "We also defined our predicted targets for each training example. This looks like:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "   0 \\\\\n",
        "   1 \\\\\n",
        "   1 \\\\\n",
        "   0 \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "So this would mean that in our training data, sample class `(0,0)` has a predicted output of `0` and so forth. In our real world example, this could correspond to the set of binary classes in our overweight example (i.e. 1 would represent overweight, and 0 not overweight)."
      ],
      "metadata": {
        "id": "hMU3l_jKd3lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialising the weight matrices\n",
        "\n",
        "We'll use numpy's `np.random.normal` function to define a `(2,3)` matrix for the input to hidden layer weights - filled with random values that fit a normal distribution with mean `0` and standard deviation `1/sqrt(n)` where n is the number of input nodes.\n",
        "\n",
        "This will produce a matrix with the following shape:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "   w_1 & w_2 & w_3 \\\\\n",
        "   w_4 & w_5 & w_6\n",
        "\\end{bmatrix}\n",
        "\n",
        "The matrix for the hidden to output layer weights uses the same method but with different dimensions, producing a weight matrix which looks like:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "   w_1 \\\\\n",
        "   w_2 \\\\\n",
        "   w_3\n",
        "\\end{bmatrix}"
      ],
      "metadata": {
        "id": "_92X5sCy--TR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_to_hidden_weight_matrix = np.random.normal(0.0, 2**-0.5,\n",
        "                                                        (2, 3))\n",
        "print(input_to_hidden_weight_matrix)\n",
        "\n",
        "hidden_to_output_weight_matrix = np.random.normal(0.0, 3**-0.5,\n",
        "                                                        (3, 1))\n",
        "\n",
        "print(hidden_to_output_weight_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YlZD30R_bKc",
        "outputId": "4cdc6091-4f63-44dd-ac10-8820443439f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.16907713 -0.52980406 -0.40294388]\n",
            " [ 1.24127819 -0.23089244  1.16215273]]\n",
            "[[-0.72198837]\n",
            " [-0.68314498]\n",
            " [ 0.21095515]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "First, we'll do a **forward pass** which specifies how the input data is transformed by the model to produce the output predictions.\n",
        "\n",
        "In our model, we do this by calculating the signals into the hidden layer by multiplying the weights by the inputs and then apply the activation function to the signals.\n",
        "\n",
        "This is simply matrix multiplication of the input matrix with our **input to hidden layer** weight matrix.\n",
        "\n",
        "i.e. \n",
        "\n",
        "$\\begin{bmatrix} x_1 & x_2 \\\\ x_3 & x_4 \\\\ x_5 & x_6 \\\\ x_7 & x_8 \\end{bmatrix}\\ \\times \\begin{bmatrix} w_1 & w_2 & w_3 \\\\ w_4 & w_5 & w_6 \\end{bmatrix} = \\begin{bmatrix} x_1w_1 + x_2w_4 & x_1w_2 + x_2w_5 & x_1w_3 + x_2w_6 \\\\ x_3w_1 + x_4w_4 & x_3w_2 + x_4w_5 & x_3w_3 + x_4w_6 \\\\ x_5w_1 + x_6w_4 & x_5w_2 + x_6w_5 & x_5w_3 + x_6w_6 \\\\ x_7w_1 + x_8w_4 & x_7w_2 + x_8w_5 & x_7w_3 + x_8w_6 \\end{bmatrix}$\n",
        "\n",
        "Then applying the activation function (sigmoid) to the results to get the full signal values.\n",
        "\n",
        "We then do the same thing to get from our hidden layer values to our output layer. "
      ],
      "metadata": {
        "id": "XO9FNJLORg9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define the activation function (sigmoid)\n",
        "def activation(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# let's plot to visualise how this looks\n",
        "\n",
        "# generate a range of input values\n",
        "x = np.linspace(-10, 10, 100)\n",
        "\n",
        "# compute the output of the neuron for each input\n",
        "y = activation(x)\n",
        "\n",
        "# plot the input-output curve\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.show()\n",
        "\n",
        "# forward pass to get hidden layer outputs (Inputs x Hidden layer Weights)\n",
        "hidden_inputs = np.matmul(train_inputs, input_to_hidden_weight_matrix)\n",
        "hidden_outputs = activation(hidden_inputs)\n",
        "\n",
        "# propagate forward to get the final layer outputs (Hidden layer outputs x Final layer Weights)\n",
        "final_inputs = np.matmul(hidden_outputs, hidden_to_output_weight_matrix)\n",
        "final_outputs = activation(final_inputs)\n",
        "\n",
        "print(hidden_outputs)\n",
        "print(final_outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "Q7LQyJ-BtpzC",
        "outputId": "e6fe301c-9f97-4b20-d2e9-7b3bfcd44907"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5        0.5        0.5       ]\n",
            " [0.77578642 0.44253197 0.76172366]\n",
            " [0.45783113 0.37056259 0.40060525]\n",
            " [0.74501527 0.31849507 0.68118194]]\n",
            "[[0.35500994]\n",
            " [0.33142817]\n",
            " [0.37773175]\n",
            " [0.35165513]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we've done our forward pass and calcuated signals all the way across our layers into our output layer. We can do a **backward pass**.\n",
        "\n",
        "This is where we calculate the **loss** (measures the difference between the predicted output and the true output) and then update the model parameters based on the loss value.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gOiK3B_mTPxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = final_outputs - train_outputs # how wrong was the model in its first pass\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls6056JPWU2n",
        "outputId": "7447aadb-51d8-4b27-a3ea-c7efac5a291b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.35500994]\n",
            " [-0.66857183]\n",
            " [-0.62226825]\n",
            " [ 0.35165513]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then update the weights using gradient descent / backpropagation. The weights are updated by multiplying the error by the inputs, then scaling by the learning rate (how quickly or slowly to update).\n",
        "\n",
        "`output_errors * final_outputs * (1.0 - final_outputs)` is the gradient & the learning rate is the step size for the gradient descent.\n",
        "\n",
        "So in full we get the updated weights to the hidden -> output layer by doing:\n",
        "\n",
        "`updated_weights += learning_rate * (forward_pass_signal_outputs * gradient)`\n",
        "\n",
        "which looks like this in our case:\n",
        "\n",
        "`weights_hidden_to_output += learning_rate * np.matmul(hidden_outputs.T, output_errors * final_outputs * (1.0 - final_outputs))`\n",
        "\n",
        "& similarly for the hidden -> input layer:\n",
        "\n",
        "`weights_input_to_hidden += learning_rate * np.matmul(inputs_list.T, hidden_errors * hidden_outputs * (1.0 - hidden_outputs))`\n"
      ],
      "metadata": {
        "id": "oWn9aTiJXEAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting this all together\n",
        "\n",
        "See below for a class which combines the above to create a simple dumbed-down neural network applying the above components."
      ],
      "metadata": {
        "id": "Vx34T2noYtv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork():\n",
        "    def __init__(self):\n",
        "        print('Initialising the model')\n",
        "\n",
        "        # number of nodes in the input, hidden and output layers\n",
        "        self.input_nodes = 2\n",
        "        self.hidden_nodes = 3\n",
        "        self.output_nodes = 1\n",
        "\n",
        "        # Defining the weights\n",
        "        # This matrix has dimensions: (input_nodes, hidden_nodes) or (2,3)\n",
        "\n",
        "        # We use a normal distribution with mean 0 and standard deviation 1/sqrt(n)\n",
        "        # where n is the number of input nodes\n",
        "        # the third argument is a tuple of the dimensions of the matrix\n",
        "        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5,\n",
        "                                                        (self.input_nodes, self.hidden_nodes))\n",
        "\n",
        "        # this is a 3x1 matrix (hidden_nodes, output_nodes)\n",
        "        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5,\n",
        "                                                         (self.hidden_nodes, self.output_nodes))\n",
        "\n",
        "        # learning rate\n",
        "        # this is the step size for the gradient descent or simply put the amount by which the weights are updated\n",
        "        # value is typically between 0.1 and 0.001\n",
        "        # if the value is too high, the weights will overshoot the minimum & if the value is too low, the weights will take a long time to converge\n",
        "        self.lr = 0.1\n",
        "\n",
        "    def train(self, inputs_list, targets_list):\n",
        "        # Forward pass\n",
        "\n",
        "        # calculate the signals into the hidden layer by multiplying the weights by the inputs\n",
        "        # then apply the activation function to the signals\n",
        "        # input_list is a 4x2 matrix and weights_input_to_hidden is a 2x3 matrix\n",
        "        # so the result is a 4x3 matrix\n",
        "        hidden_inputs = np.matmul(inputs_list, self.weights_input_to_hidden)\n",
        "        hidden_outputs = self.sigmoid(hidden_inputs)\n",
        "\n",
        "        # calculate the signals into the final output layer\n",
        "        # multiply the hidden layer outputs by the weights\n",
        "        # then apply the activation function\n",
        "        # hidden_outputs is a 4x3 matrix and weights_hidden_to_output is a 3x1 matrix\n",
        "        # so the result is a 4x1 matrix\n",
        "        final_inputs = np.matmul(hidden_outputs, self.weights_hidden_to_output)\n",
        "        final_outputs = self.sigmoid(final_inputs)\n",
        "\n",
        "        # Backward pass\n",
        "\n",
        "        # First, calculate the error\n",
        "\n",
        "        # error is (target - actual)\n",
        "        # to get the hidden layer error, we need to multiply the output error by the weights\n",
        "        # hidden layer error is the output error, split by the weights, recombined at the hidden nodes\n",
        "        # weights_hidden_to_output is a 3x1 matrix and output_errors is a 4x1 matrix\n",
        "        # so the result is a 4x3 matrix\n",
        "        output_errors = targets_list - final_outputs\n",
        "        hidden_errors = np.matmul(\n",
        "            output_errors, self.weights_hidden_to_output.T)\n",
        "\n",
        "        # update the weights using gradient descent / backpropagation\n",
        "        # the weights are updated by multiplying the error by the inputs, then scaling by the learning rate\n",
        "        # output_errors * final_outputs * (1.0 - final_outputs) is the gradient & the learning rate is the step size for the gradient descent\n",
        "        # the hidden to output weights are updated by multiplying the error by the hidden layer outputs\n",
        "        # the input to hidden weights are updated by multiplying the error by the inputs\n",
        "        # hidden_outputs is a 4x3 matrix and output_errors * final_outputs * (1.0 - final_outputs) is a 3x1 matrix\n",
        "        # so we transpose the hidden_outputs matrix to make it a 3x4 matrix and then multiply it by the 4x1 matrix\n",
        "        # so the result is a 3x1 matrix\n",
        "        self.weights_hidden_to_output += self.lr * \\\n",
        "            np.matmul(hidden_outputs.T, output_errors *\n",
        "                      final_outputs * (1.0 - final_outputs))\n",
        "\n",
        "        # inputs_list is a 4x2 matrix and hidden_errors * hidden_outputs * (1.0 - hidden_outputs) is a 4x3 matrix\n",
        "        # so we transpose the inputs_list matrix to make it a 2x4 matrix and then multiply it by the 4x3 matrix\n",
        "        # so the result is a 2x3 matrix\n",
        "        self.weights_input_to_hidden += self.lr * \\\n",
        "            np.matmul(inputs_list.T, hidden_errors *\n",
        "                      hidden_outputs * (1.0 - hidden_outputs))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        output = 1 / (1 + np.exp(-x))\n",
        "        return output\n",
        "\n",
        "    def predict(self, input):\n",
        "        # to make a prediction, we need to forward pass the input through the network\n",
        "        # the input is a 1x2 matrix and weights_input_to_hidden is a 2x3 matrix\n",
        "        # so the result is a 1x3 matrix\n",
        "        hidden_inputs = np.matmul(\n",
        "            np.array(input), self.weights_input_to_hidden)\n",
        "\n",
        "        # input is a 1x2 matrix and weights_input_to_hidden is a 2x3 matrix\n",
        "        # so the result is a 1x3 matrix\n",
        "        # calculate the signals into the hidden layer\n",
        "        hidden_inputs = np.matmul(\n",
        "            np.array(input), self.weights_input_to_hidden)\n",
        "\n",
        "        # calculate the signals emerging from the hidden layer\n",
        "        # we don't need to multiply by the weights because the weights are already in the hidden_inputs so we just apply the activation function\n",
        "        hidden_outputs = self.sigmoid(hidden_inputs)\n",
        "\n",
        "        # calculate the signals into the final output layer\n",
        "        # hidden_outputs is a 1x3 matrix and weights_hidden_to_output is a 3x1 matrix\n",
        "        # so the result is a 1x1 matrix\n",
        "        final_inputs = np.matmul(hidden_outputs, self.weights_hidden_to_output)\n",
        "        final_outputs = self.sigmoid(final_inputs)\n",
        "\n",
        "        return final_outputs"
      ],
      "metadata": {
        "id": "roTUhzx8VhPK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first initialise how customer simplified neural network"
      ],
      "metadata": {
        "id": "c_fakfwFY-oR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAZJMG6cZC3K",
        "outputId": "69568158-5711-4e23-e46d-ebcaa168f6b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialising the model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now train it based on our fake training data"
      ],
      "metadata": {
        "id": "LZD4uoOTajMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    model.train(train_inputs, train_outputs)"
      ],
      "metadata": {
        "id": "D_jZq1tKanDl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally use our trained model to make a prediction based on a class with data: `(1,0)`"
      ],
      "metadata": {
        "id": "3FO1UeeAbQ6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.predict([1,0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbxBeyYQbTZH",
        "outputId": "8e76904e-8299-4bef-832a-faa86f01f047"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.53795434]\n"
          ]
        }
      ]
    }
  ]
}